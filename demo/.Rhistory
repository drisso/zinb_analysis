alpha0 <- rnorm(n, mean=2, sd=2)
alpha1 <- rnorm(n, mean=-1, sd=0.3)
alpha2 <- rnorm(n, mean=0, sd=0.15)
alpha3 <- rnorm(n, mean=1.15, sd=2)
logistic <- binomial()$linkinv
## generate pi = prob. of "detection"
pi <- sapply(1:n , function(i) {
eta <- alpha0[i] + alpha1[i] * log(mu_pop) #+ alpha2[i] * log(l) + alpha3[i] * g
return(logistic(eta))
})
## generate Z, indicator of expression
Z <- matrix(rbinom(n*J, 1, pi), ncol=n)
head(Z)
## generate Y, read counts (same dispersion for all genes)
Y <- matrix(data=0, ncol=n, nrow=J)
Y[Z==0] <- rnbinom(sum(Z==0), mu = mu[Z==0], size = 1)
head(Y)
table(rowSums(Y)>0)
which(rowSums(Y)==0)
rowSums(Y)
colSums(Y)
Y <- Y[rowSums(Y)>0, colSums(Y)>0]
dim(Y)
head(Y)
parameters <- c(log(mu_pop), alpha0, alpha1, rep(log(1), J))
X <- matrix(rep(1, n), ncol=1)
dim(X)
W <- model.matrix(~log(mu_pop))
dim(W)
nbeta <- J
nalpha <- n*2
loglik_small(parameters, Y, Y>0, X, W, nbeta, nalpha, 0, 0, binomial())
grad_small(parameters, Y, Y>0, X, W, nbeta, nalpha, 0, 0, binomial())
## optimize the likelihood with optim, starting from the true values
system.time(fit <- optim(fn = loglik_small, gr = grad_small, par = parameters, Y=Y, Y1=Y>0, X=X, W=W,
kx=nbeta, kw=nalpha, offsetx=0, offsetw=0, linkobj=binomial(),
hessian = FALSE, method = "BFGS", control=list(fnscale=-1)))
betahat <- matrix(fit$par[1:nbeta], ncol=J, nrow=ncol(X))
muhat2 <- t(exp(X %*% betahat))[,1]
alphahat <- matrix(fit$par[(nbeta + 1):(nbeta + nalpha)], nrow=ncol(W), ncol=n, byrow=TRUE)
etahat <- W %*% alphahat
pihat2 <- logistic(etahat)
J <- 100
n <- 10
mu_pop <- sample(means, J)
## homogeneous population == same mu across all samples
mu <- matrix(data=rep(mu_pop, n), ncol=n, nrow=J)
## sample from real GCC and length
l <- len[names(mu_pop)]
g <- gcc[names(mu_pop)]
## values based on BRAIN data
alpha0 <- rnorm(n, mean=2, sd=2)
alpha1 <- rnorm(n, mean=-1, sd=0.3)
alpha2 <- rnorm(n, mean=0, sd=0.15)
alpha3 <- rnorm(n, mean=1.15, sd=2)
logistic <- binomial()$linkinv
## generate pi = prob. of "detection"
pi <- sapply(1:n , function(i) {
eta <- alpha0[i] + alpha1[i] * log(mu_pop) #+ alpha2[i] * log(l) + alpha3[i] * g
return(logistic(eta))
})
## generate Z, indicator of expression
Z <- matrix(rbinom(n*J, 1, pi), ncol=n)
head(Z)
## generate Y, read counts (same dispersion for all genes)
Y <- matrix(data=0, ncol=n, nrow=J)
Y[Z==0] <- rnbinom(sum(Z==0), mu = mu[Z==0], size = 1)
head(Y)
parameters <- c(log(mu_pop), alpha0, alpha1, rep(log(1), J))
X <- matrix(rep(1, n), ncol=1)
dim(X)
W <- model.matrix(~log(mu_pop))
dim(W)
nbeta <- J
nalpha <- n*2
loglik_small(parameters, Y, Y>0, X, W, nbeta, nalpha, 0, 0, binomial())
grad_small(parameters, Y, Y>0, X, W, nbeta, nalpha, 0, 0, binomial())
## optimize the likelihood with optim, starting from the true values
system.time(fit <- optim(fn = loglik_small, gr = grad_small, par = parameters, Y=Y, Y1=Y>0, X=X, W=W,
kx=nbeta, kw=nalpha, offsetx=0, offsetw=0, linkobj=binomial(),
hessian = FALSE, method = "BFGS", control=list(fnscale=-1)))
betahat <- matrix(fit$par[1:nbeta], ncol=J, nrow=ncol(X))
muhat2 <- t(exp(X %*% betahat))[,1]
alphahat <- matrix(fit$par[(nbeta + 1):(nbeta + nalpha)], nrow=ncol(W), ncol=n, byrow=TRUE)
etahat <- W %*% alphahat
pihat2 <- logistic(etahat)
fit2 <- em(Y)
i += 1
i +=1
mu0 <- rowMeans(Y)
pi0 <- apply(Y, 2, function(x) {
y <- as.numeric(x<=0)
fit <- glm(y~log(mu0), family = binomial)
return(fitted.values(fit))
})
pi0
zhat <- pi0/(pi0 + (1 - pi0) * dnbinom(0, size = 1, mu = matrix(mu0, nrow=J, ncol=n)))
zhat[Y>0] <- 0
thetahat <- rep(1, J)
coefs_mu <- log(mu0)
coefs_pi <- sapply(seq_len(n), function(i) {
fit <- suppressWarnings(glm(zhat[,i]~log(mu0), family = binomial(link = logit)))
return(coefficients(fit))
})
X <- matrix(rep(1, n), ncol=1)
W <- model.matrix(~log(mu0))
linkobj <- binomial()
ll_new <- loglik_small(c(coefs_mu, coefs_pi[1,], coefs_pi[2,], log(thetahat)), Y, Y>0, X, W, J, n*2, 0, 0, linkobj)
ll_old <- 2 * ll_new
ll_new
while (abs((ll_old - ll_new)/ll_old) > 1e-4) {
ll_old <- ll_new
fit_mu <- lapply(seq_len(J), function(i) {
fit <- suppressWarnings(glm.nb(Y[i,] ~ 1, weights = (1 - zhat[i,]), init.theta = thetahat[i], start=coefs_mu[i]))
return(fit)
})
coefs_mu <- sapply(fit_mu, coefficients)
muhat <- exp(coefs_mu)
thetahat <- sapply(fit_mu, function(x) x$theta)
fit_pi <- lapply(seq_len(n), function(i) {
fit <- suppressWarnings(glm(zhat[,i]~log(muhat), family = binomial(link = logit), start=coefs_pi[,i]))
return(fit)
})
coefs_pi <- sapply(fit_pi, coefficients)
pihat <- sapply(fit_pi, fitted.values)
zhat <- pihat/(pihat + (1 - pihat) * dnbinom(0, size = matrix(thetahat, nrow=J, ncol=n), mu = matrix(muhat, nrow=J, ncol=n)))
zhat[Y>0] <- 0
W <- model.matrix(~log(muhat))
ll_new <- loglik_small(c(coefs_mu, coefs_pi[1,], coefs_pi[2,], log(thetahat)), Y, Y>0, X, W, J, n*2, 0, 0, linkobj)
print(ll_new)
}
maxiter = 100
iter = 0
while (abs((ll_old - ll_new)/ll_old) > 1e-4 & iter<maxiter) {
ll_old <- ll_new
fit_mu <- lapply(seq_len(J), function(i) {
fit <- suppressWarnings(glm.nb(Y[i,] ~ 1, weights = (1 - zhat[i,]), init.theta = thetahat[i], start=coefs_mu[i]))
return(fit)
})
coefs_mu <- sapply(fit_mu, coefficients)
muhat <- exp(coefs_mu)
thetahat <- sapply(fit_mu, function(x) x$theta)
fit_pi <- lapply(seq_len(n), function(i) {
fit <- suppressWarnings(glm(zhat[,i]~log(muhat), family = binomial(link = logit), start=coefs_pi[,i]))
return(fit)
})
coefs_pi <- sapply(fit_pi, coefficients)
pihat <- sapply(fit_pi, fitted.values)
zhat <- pihat/(pihat + (1 - pihat) * dnbinom(0, size = matrix(thetahat, nrow=J, ncol=n), mu = matrix(muhat, nrow=J, ncol=n)))
zhat[Y>0] <- 0
W <- model.matrix(~log(muhat))
ll_new <- loglik_small(c(coefs_mu, coefs_pi[1,], coefs_pi[2,], log(thetahat)), Y, Y>0, X, W, J, n*2, 0, 0, linkobj)
print(ll_new)
iter <- iter + 1
}
ietr
iter
maxiter
iter<maxiter
ll_old
ll_new
ll_new <- loglik_small(c(coefs_mu, coefs_pi[1,], coefs_pi[2,], log(thetahat)), Y, Y>0, X, W, J, n*2, 0, 0, linkobj)
ll_old <- 2 * ll_new
## EM iteration
iter <- 0
while (abs((ll_old - ll_new)/ll_old) > 1e-4 & iter<maxiter) {
ll_old <- ll_new
fit_mu <- lapply(seq_len(J), function(i) {
fit <- suppressWarnings(glm.nb(Y[i,] ~ 1, weights = (1 - zhat[i,]), init.theta = thetahat[i], start=coefs_mu[i]))
return(fit)
})
coefs_mu <- sapply(fit_mu, coefficients)
muhat <- exp(coefs_mu)
thetahat <- sapply(fit_mu, function(x) x$theta)
fit_pi <- lapply(seq_len(n), function(i) {
fit <- suppressWarnings(glm(zhat[,i]~log(muhat), family = binomial(link = logit), start=coefs_pi[,i]))
return(fit)
})
coefs_pi <- sapply(fit_pi, coefficients)
pihat <- sapply(fit_pi, fitted.values)
zhat <- pihat/(pihat + (1 - pihat) * dnbinom(0, size = matrix(thetahat, nrow=J, ncol=n), mu = matrix(muhat, nrow=J, ncol=n)))
zhat[Y>0] <- 0
W <- model.matrix(~log(muhat))
ll_new <- loglik_small(c(coefs_mu, coefs_pi[1,], coefs_pi[2,], log(thetahat)), Y, Y>0, X, W, J, n*2, 0, 0, linkobj)
print(ll_new)
iter <- iter + 1
}
iter
maxiter
?optim
muhat
plot(muhat~mu)
plot(muhat~mu_pop)
plot(muhat~mu_pop, log='xy')
source("../R/em.R")
fit2 <- em(Y)
boxplot(data.frame(EM=as.numeric(fit2$pihat - pi), optim=as.numeric(pihat2 - pi)), main="Pi")
abline(h=0, col=2)
boxplot(data.frame(EM=log(muhat), optim=log(muhat2)) - log(mu_pop), main="Mu")
abline(h=0, col=2)
boxplot(data.frame(EM=log(fit2$muhat), optim=log(muhat2)) - log(mu_pop), main="Mu")
abline(h=0, col=2)
boxplot(data.frame(EM=1/fit2$thetahat, optim=1/fit$par[length(fit$par)]) - 1, main="Phi")
fit$par
fit$par[(nalpha + nbeta + 1)]
fit$par[(nalpha + nbeta + 1):length(fit$par)]
length(fit$par[(nalpha + nbeta + 1):length(fit$par)])
thetahat2 <- exp(fit$par[(nalpha + nbeta + 1):length(fit$par)])
1/thetahat2
round(1/thetahat2)
thetahat2 <- fit$par[(nalpha + nbeta + 1):length(fit$par)]
table(thetahat2>0)
thetahat2
thetahat2 <- exp(fit$par[(nalpha + nbeta + 1):length(fit$par)])
sim2$thetahat
fit2$thetahat
round(fit2$thetahat)
round(thetahat2)
boxplot(data.frame(EM=1/fit2$thetahat, optim=1/thetahat2) - 1, main="Phi")
abline(h=0, col=2)
plot(1-pihat[order(muhat),1]~sort(log(muhat)), type='l', ylim=c(0, 1))
for(i in 2:n) {
lines(1-pihat[order(muhat),i]~sort(log(muhat)))
}
hist(pihat[,1])
hist(log(Y[,1]+0.1))
hist(pihat[,1])
hist(log(Y[,1]+0.1))
hist(pihat[,2])
hist(log(Y[,2]+0.1))
alpha0
alpha1
parameters <- c(log(rowMeans(Y)), rep(1, n), rep(-1, n), rep(log(1), J))
X <- matrix(rep(1, n), ncol=1)
dim(X)
W <- model.matrix(~log(mu_pop))
dim(W)
parameters <- c(log(rowMeans(Y)), rep(1, n), rep(-1, n), rep(log(1), J))
X <- matrix(rep(1, n), ncol=1)
dim(X)
W <- model.matrix(~log(mu_pop))
dim(W)
nbeta <- J
nalpha <- n*2
loglik_small(parameters, Y, Y>0, X, W, nbeta, nalpha, 0, 0, binomial())
grad_small(parameters, Y, Y>0, X, W, nbeta, nalpha, 0, 0, binomial())
system.time(fit <- optim(fn = loglik_small, gr = grad_small, par = parameters, Y=Y, Y1=Y>0, X=X, W=W,
kx=nbeta, kw=nalpha, offsetx=0, offsetw=0, linkobj=binomial(),
hessian = FALSE, method = "BFGS", control=list(fnscale=-1)))
print(system.time(fit <- optim(fn = loglik_small, gr = grad_small, par = parameters, Y=Y, Y1=Y>0, X=X, W=W,
kx=nbeta, kw=nalpha, offsetx=0, offsetw=0, linkobj=binomial(),
hessian = FALSE, method = "BFGS", control=list(fnscale=-1))))
betahat <- matrix(fit$par[1:nbeta], ncol=J, nrow=ncol(X))
muhat2 <- t(exp(X %*% betahat))[,1]
alphahat <- matrix(fit$par[(nbeta + 1):(nbeta + nalpha)], nrow=ncol(W), ncol=n, byrow=TRUE)
etahat <- W %*% alphahat
pihat2 <- logistic(etahat)
thetahat2 <- exp(fit$par[(nalpha + nbeta + 1):length(fit$par)])
fit2 <- em(Y)
boxplot(data.frame(EM=as.numeric(fit2$pihat - pi), optim=as.numeric(pihat2 - pi)), main="Pi")
abline(h=0, col=2)
boxplot(data.frame(EM=log(fit2$muhat), optim=log(muhat2)) - log(mu_pop), main="Mu")
abline(h=0, col=2)
boxplot(data.frame(EM=1/fit2$thetahat, optim=1/thetahat2) - 1, main="Phi")
abline(h=0, col=2)
plot(1-pihat[order(muhat),1]~sort(log(muhat)), type='l', ylim=c(0, 1))
for(i in 2:n) {
lines(1-pihat[order(muhat),i]~sort(log(muhat)))
}
hist(pihat[,1])
hist(log(Y[,1]+0.1))
hist(pihat[,2])
hist(log(Y[,2]+0.1))
print(system.time(fit2 <- em(Y)))
boxplot(data.frame(EM=as.numeric(fit2$pihat - pi), optim=as.numeric(pihat2 - pi)), main="Pi")
abline(h=0, col=2)
boxplot(data.frame(EM=as.numeric(fit2$pihat - pi), optim=as.numeric(pihat2 - pi)), main="Pi", ylab="pihat - pi")
plot(1-pihat[order(muhat),1]~sort(log(muhat)), type='l', ylim=c(0, 1))
plot(1-pihat[order(muhat),1]~sort(log(muhat)), type='l', ylim=c(0, 1), ylab="1 - pihat", xlab="log(muhat)")
for(i in 2:n) {
lines(1-pihat[order(muhat),i]~sort(log(muhat)))
}
dim(Y)
optimize <- function(Y, mu_pop) {
## optim (assuming we know mu)
parameters <- c(log(rowMeans(Y)), rep(1, n), rep(-1, n), rep(log(1), J))
X <- matrix(rep(1, n), ncol=1)
dim(X)
W <- model.matrix(~log(mu_pop))
dim(W)
nbeta <- J
nalpha <- n*2
print(system.time(fit <- optim(fn = loglik_small, gr = grad_small, par = parameters, Y=Y, Y1=Y>0, X=X, W=W, kx=nbeta, kw=nalpha, offsetx=0, offsetw=0, linkobj=binomial(), hessian = FALSE, method = "BFGS", control=list(fnscale=-1))))
betahat <- matrix(fit$par[1:nbeta], ncol=J, nrow=ncol(X))
muhat <- t(exp(X %*% betahat))[,1]
alphahat <- matrix(fit$par[(nbeta + 1):(nbeta + nalpha)], nrow=ncol(W), ncol=n, byrow=TRUE)
etahat <- W %*% alphahat
pihat <- logistic(etahat)
thetahat <- exp(fit$par[(nalpha + nbeta + 1):length(fit$par)])
return(list(muhat=muhat, pihat=pihat, thetahat=thetahat))
}
print(system.time(fit1 <- optimize(Y, mu_pop)))
optimize <- function(Y, mu_pop) {
## optim (assuming we know mu)
parameters <- c(log(rowMeans(Y)), rep(1, n), rep(-1, n), rep(log(1), J))
X <- matrix(rep(1, n), ncol=1)
dim(X)
W <- model.matrix(~log(mu_pop))
dim(W)
nbeta <- J
nalpha <- n*2
fit <- optim(fn = loglik_small, gr = grad_small, par = parameters, Y=Y, Y1=Y>0, X=X, W=W, kx=nbeta, kw=nalpha, offsetx=0, offsetw=0, linkobj=binomial(), hessian = FALSE, method = "BFGS", control=list(fnscale=-1))
betahat <- matrix(fit$par[1:nbeta], ncol=J, nrow=ncol(X))
muhat <- t(exp(X %*% betahat))[,1]
alphahat <- matrix(fit$par[(nbeta + 1):(nbeta + nalpha)], nrow=ncol(W), ncol=n, byrow=TRUE)
etahat <- W %*% alphahat
pihat <- logistic(etahat)
thetahat <- exp(fit$par[(nalpha + nbeta + 1):length(fit$par)])
return(list(muhat=muhat, pihat=pihat, thetahat=thetahat))
}
print(system.time(fit1 <- optimize(Y, mu_pop)))
print(system.time(fit2 <- em(Y)))
J <- 1000
n <- 10
mu_pop <- sample(means, J)
## homogeneous population == same mu across all samples
mu <- matrix(data=rep(mu_pop, n), ncol=n, nrow=J)
## sample from real GCC and length
l <- len[names(mu_pop)]
g <- gcc[names(mu_pop)]
## values based on BRAIN data
alpha0 <- rnorm(n, mean=2, sd=2)
alpha1 <- rnorm(n, mean=-1, sd=0.3)
alpha2 <- rnorm(n, mean=0, sd=0.15)
alpha3 <- rnorm(n, mean=1.15, sd=2)
logistic <- binomial()$linkinv
## generate pi = prob. of "detection"
pi <- sapply(1:n , function(i) {
eta <- alpha0[i] + alpha1[i] * log(mu_pop) #+ alpha2[i] * log(l) + alpha3[i] * g
return(logistic(eta))
})
## generate Z, indicator of expression
Z <- matrix(rbinom(n*J, 1, pi), ncol=n)
## generate Y, read counts (same dispersion for all genes)
Y <- matrix(data=0, ncol=n, nrow=J)
Y[Z==0] <- rnbinom(sum(Z==0), mu = mu[Z==0], size = 1)
parameters <- c(log(rowMeans(Y)), rep(1, n), rep(-1, n), rep(log(1), J))
X <- matrix(rep(1, n), ncol=1)
dim(X)
W <- model.matrix(~log(mu_pop))
dim(W)
nbeta <- J
nalpha <- n*2
fit <- optim(fn = loglik_small, gr = grad_small, par = parameters, Y=Y, Y1=Y>0, X=X, W=W, kx=nbeta, kw=nalpha, offsetx=0, offsetw=0, linkobj=binomial(), hessian = FALSE, method = "BFGS", control=list(fnscale=-1))
loglik_small(parameters, Y, Y>0, X, W, nbeta, nalpha, 0, 0, binomial())
grad_small(parameters, Y, Y>0, X, W, nbeta, nalpha, 0, 0, binomial())
print(system.time(fit2 <- em(Y)))
Q
boxplot(log(filtered+1), outline=FALSE)
class(filtered)
print(system.time(fit_norm <- em(filtered)))
source('~/git/zinb/R/em.R', echo=TRUE)
print(system.time(fit_norm <- em(filtered)))
class(filtered)
debug(filtered)
debug(em)
print(system.time(fit_norm <- em(filtered)))
J
pi0
dim(pi0)
print(system.time(fit_norm <- em(filtered)))
dim(pi0)
dim(mu0)
length(mu0)
pi0
pi0/(pi0 + (1 - pi0)
)
dnbinom(0, size = 1, mu = matrix(mu0, nrow=J, ncol=n)
)
J
n <- ncol(Y)
J <- nrow(Y)
J
n
source('~/git/zinb/R/em.R', echo=TRUE)
print(system.time(fit_norm <- em(filtered)))
mu0
Y <- filtered
n <- ncol(Y)
J <- nrow(Y)
mu0 <- rowMeans(Y)
pi0 <- apply(Y, 2, function(x) {
y <- as.numeric(x<=0)
fit <- glm(y~log(mu0), family = binomial)
return(fitted.values(fit))
})
zhat <- pi0/(pi0 + (1 - pi0) * dnbinom(0, size = 1, mu = matrix(mu0, nrow=J, ncol=n)))
zhat[Y>0] <- 0
thetahat <- rep(1, J)
thetahat
zhat
pi0
coefs_mu <- log(mu0)
coefs_pi <- sapply(seq_len(n), function(i) {
fit <- suppressWarnings(glm(zhat[,i]~log(mu0), family = binomial(link = logit)))
return(coefficients(fit))
})
coefs_pi
X <- matrix(rep(1, n), ncol=1)
W <- model.matrix(~log(mu0))
linkobj <- binomial()
ll_new <- loglik_small(c(coefs_mu, coefs_pi[1,], coefs_pi[2,], log(thetahat)), Y, Y>0, X, W, J, n*2, 0, 0, linkobj)
ll_new
coefs_mu
coefs_pi[1,]
coefs_pi[2,]
log(thetahat)
Y
Y>)
Y>0
X
W
J
n*2
ll_old <- 2 * ll_new
abs((ll_old - ll_new)/ll_old)
abs((ll_old - ll_new)/ll_old) > 1e-4
abs((ll_old - ll_new)/ll_old) > 1e-4 & iter<maxiter
set.seed(12324)
n=100
# Number of genes
J=60
# Number of latent actors
p=2
# Generate the n*p matrix U of p latent factors for the cells
U=cbind(runif(n),runif(n))
# TODO: U has only two columns, however the number of latent factors is p. You should define U with p columns so that the code will work when we change p to some other values. For example:
# U <- matrix(runif(n*p),nrow=n)
#Simulate true U-coefficients for M
# TODO: same comment as for U, V should have p rows
V=rbind(runif(J),runif(J))
#design matrix X.M
X.M=matrix(0,nrow=n,ncol=p)
# TODO: I'm lost, here M has p columns, I thought p was for the latent factors in U and V?
X.M[,1]=c(rep(1,n/2),rep(0,n/2))
X.M[,2]=c(rep(0,n/2),rep(1,n/2))
#overall design matrix for M
X=cbind(X.M,U)
#coefficients for X.M
alpha.M=matrix(0,nrow=p,ncol=J)
alpha.M[1,]=5*runif(J)
alpha.M[2,]=6*runif(J)
#matrix M
M=exp(X.M%*%alpha.M+U%*%V)
#simulate true U-coefficients for Pi
W=-rbind(c(rep(1,J/2),rep(2,J/2)),c(rep(4,J/2),rep(3,J/2)))
#design matrix X.pi, take the same for the moment
X.pi=matrix(0,nrow=n,ncol=p)
X.pi[,1]=c(rep(1,n/2),rep(0,n/2))
X.pi[,2]=c(rep(0,n/2),rep(1,n/2))
#true coefficients for X.pi
alpha.pi=rbind(c(rep(0.01,J/2),rep(0.005,J/2)),c(rep(0.007,J/2),rep(0.02,J/2)))
#matrix of probabilities of dropout
Pi=binomial()$linkinv(X.pi%*%alpha.pi+U%*%W)
min(Pi)
#simulate negative binomial from M (matrix of expressions)
exprs=NULL
for(i in 1:(n*J)){
exprs[i]=rnbinom(1,mu=as.vector(M)[i],size=log(3))
}
exprs=matrix(exprs,nrow=n)
indic=NULL
for(i in 1:(n*J)){
indic[i]=1-rbinom(1,size=1,prob=as.vector(Pi)[i])
}
indic=matrix(indic,nrow=n)
Y=matrix(0,nrow=n,ncol=J)
Y[indic==T]=exprs[indic==T]
sum(Y==0)
#which(apply(Y==0,2,sum)==max(apply(Y==0,2,sum)))
#sum(Y[,17]==0)
sum(apply(Y!=0,2,sum)==0)
######################### end of data simulation  ##########################
#########################  beginning of the model estimation  ##############
#X_M design matrix for M, X_pi same for Pi
#U latent factor matrix
#matrix of covariates for M regression
#(kx=nb of known M-factors + nb of latent factors)
# TODO: X was already defined, no need to redefine it
X=cbind(X.M,U)
Z=cbind(X.pi,U)
offsetx=0 #no offset for the moment
offsetz=0 #no offset for the moment
n <- nrow(Y) #number of cells
kx <- NCOL(X) #number of M-factors
kz <- NCOL(Z) #number of Pi-factors
Y0 <- Y <= 0 #==1 if counts is 0
Y1 <- Y > 0 #==1 if count is not 0
#test of the first part : optimization only wrt the right part
trueparam=rbind(alpha.M,V,alpha.pi,W,log(6))
optim(fn=ziNegBin,gr=gradNegBin,j=10,epsilon=0,X=X,Z=Z,Y=Y,par=c(1,1,0.5,0.1,1,1,1,1,1),control=list(fnscale=-1),method="BFGS")$par
trueparam[,10]
source("../R/functions_svetlana.R")
optim(fn=ziNegBin,gr=gradNegBin,j=10,epsilon=0,X=X,Z=Z,Y=Y,par=c(1,1,0.5,0.1,1,1,1,1,1),control=list(fnscale=-1),method="BFGS")$par
source("../R/reg_smallmatrix.R")
optim(fn=ziNegBin,gr=gradNegBin,j=10,epsilon=0,X=X,Z=Z,Y=Y,par=c(1,1,0.5,0.1,1,1,1,1,1),control=list(fnscale=-1),method="BFGS")$par
trueparam=rbind(alpha.M,V,alpha.pi,W,log(6))
optim(fn=ziNegBin,gr=gradNegBin,j=10,epsilon=0,X=X,Z=Z,Y=Y,par=c(1,1,0.5,0.1,1,1,1,1,1),control=list(fnscale=-1),method="BFGS")$par
ziNegBin
gradNegBin
library(pscl)
library(zinb)
