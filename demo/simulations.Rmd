---
title: "Simulation study of the ZINB model"
output: html_document
---

I wrote two functions to simulate data : 

* sim.make.matrices functions in a similar way as zinb.make.matrices and compute logM, logitPi and logtheta from model matrices which are provided as its arguments
* simulateNB takes as argument model matrices and simulate a dataset, parallelized to simulate large data more rapidly. The number of genes and cells being determined automatically from sizes of the provided design matrices.

A small test of function simulating the data:

```{r}
source("../R/functions_sim.R")
source("../R/functions_jp.R")

# number of cells
n=10

# number of genes
J=10

# define U, V, W and logtheta
U <- matrix(rep(2,n),ncol=1)
V <- matrix(rep(1,J),nrow=1)
W <- matrix(rep(1,J),nrow=1)
# no overdispersion : logtheta = 0
logtheta <- matrix( rep ( 0 , n*J ) , ncol = J , nrow = n )

# run the function simulateNB which simulates data from given model matrices
test.simulation <- simulateNB( U = U , V = V , W = W , offset.theta = logtheta )

print(test.simulation$counts)
print(test.simulation$data.nb)
print(test.simulation$dropouts)
```

First configuration: 100 genes and 100 cells. Only U, V, W are non trivial matrices.

```{r}
# number of cells
n <- 100

# number of genes
J <- 100

# number of latent factors
k <- 2

# matrix of latent factors
U <- matrix( rep(0,n*k) , nrow = n , ncol = k )
```

### Most simple case: 2 types of cells
Define U matrix to have two groups of approx. equal size via a mixture of two gaussian

```{r}

# simulate group indicator
ind <- rbinom( n , size = 1 , prob=1/2 )

# simulate two components of mixture
U [ ind == 1 , ] <- t ( sapply ( seq ( sum(ind) ) , 
                    function (i) {abs (MASS::mvrnorm ( 1 , mu = c(2,4), Sigma = matrix ( c(1,0,0,1) , nrow=2 ) ) )} ))
U [ ind == 0 , ] <- t ( sapply ( seq ( sum(1-ind) ) , function(i) {abs (MASS::mvrnorm ( 1 , mu = c(6,8), Sigma = matrix ( c(1,0,0,1) , nrow=2 ) ) )} ) )


# define gene loadings V on columns of U
V <- matrix ( abs ( rnorm( n*k , mean = 1, sd = 1 ) ), nrow = 2 )/4

# define W matrix
W <- matrix ( rnorm( n*k , mean = -0.5, sd = 0.1 ) , nrow = 2 )

# dispersion parameter fixed between 1 and exp(1)
# logtheta <- matrix( rep ( runif(J), n) , ncol = J , byrow = TRUE )

# no overdispersion : logtheta = 0
logtheta <- matrix( rep(0,n*J) , ncol = J , byrow = TRUE )

# simulate data
simdata <- simulateNB ( U=U, V=V, W=W, offset.theta = logtheta )

# check if there are any zero columns
sum ( colSums(simdata$counts!=0) == 0 )

# plot simulated groups
plot(U)

print(simdata$zero.fraction)
```

Run zinb.PCA, zinb.PCA.theta (with known theta) and several values of penalty parameter
```{r}
simdata.zinb <- zinb.PCA (simdata$counts , no_cores=7 , alt.number=25 , verbose=FALSE)

simdata.zinb.theta.0.1 <- zinb.PCA.theta (simdata$counts , a.theta=rep(0,J), no_cores=7 , alt.number=25 , verbose=FALSE,epsilon=0.1)

simdata.zinb.theta.0.01 <- zinb.PCA.theta (simdata$counts , a.theta=rep(0,J), no_cores=7 , alt.number=25 , verbose=FALSE,epsilon=0.01)

simdata.zinb.theta.0.7 <- zinb.PCA.theta (simdata$counts , a.theta=rep(0,J), no_cores=7 , alt.number=25 , verbose=FALSE,epsilon=0.7)
```
Theta is actually overestimated. Here is the plot of estimated values:

```{r, echo =FALSE}
plot(simdata.zinb$theta,ylab="Theta estimated")
```

Now plot U times V and U times W for diffrent penalties (using known theta values).

* epsilon = 0.01

```{r, echo =FALSE}
plot(as.vector(U%*%V),as.vector(simdata.zinb.theta.0.01$U%*%t(simdata.zinb.theta.0.01$V)),ylab="UV estimated, penalty = 0.01")
plot(as.vector(U%*%W),as.vector(simdata.zinb.theta.0.01$U%*%t(simdata.zinb.theta.0.01$W)),ylab="UW estimated, penalty = 0.01")
```

* epsilon = 0.1

```{r, echo =FALSE}
plot(as.vector(U%*%V),as.vector(simdata.zinb.theta.0.1$U%*%t(simdata.zinb.theta.0.1$V)),ylab="UV estimated, penalty = 0.1")
plot(as.vector(U%*%W),as.vector(simdata.zinb.theta.0.1$U%*%t(simdata.zinb.theta.0.1$W)),ylab="UW estimated, penalty = 0.1")
```

* epsilon = 0.7

```{r, echo =FALSE}
plot(as.vector(U%*%V),as.vector(simdata.zinb.theta.0.7$U%*%t(simdata.zinb.theta.0.7$V)),ylab="UV estimated, penalty = 0.7")
plot(as.vector(U%*%W),as.vector(simdata.zinb.theta.0.7$U%*%t(simdata.zinb.theta.0.7$W)),ylab="UW estimated, penalty = 0.7")
```

And finally let us keep the unknown theta, but we take high value of penalty :

```{r}
simdata.zinb.0.7 <- zinb.PCA (simdata$counts , no_cores=7 , alt.number=25 , verbose=FALSE,epsilon=0.7)
```

```{r, echo =FALSE}
plot(as.vector(U%*%V),as.vector(simdata.zinb.0.7$U%*%t(simdata.zinb.0.7$V)),ylab="UV estimated, penalty = 0.7")
plot(as.vector(U%*%W),as.vector(simdata.zinb.0.7$U%*%t(simdata.zinb.0.7$W)),ylab="UW estimated, penalty = 0.7")
```

```{r}
# run pca
simdata.pca <- prcomp(log(1+simdata$counts), scale.=TRUE, center = TRUE)

#plot PCA
plot(simdata.pca$x[,1:2])

#simdata.pca2 <- prcomp(U%*%V, scale.=TRUE, center = TRUE)
#plot(simdata.pca2$x[,1:2])
#plot(U%*%V,simdata.zinb$U%*%t(simdata.zinb$V))
```
Now apply kmeansruns : function from fpc package, includes several runs of kmeans and detection of the optimal number of clusters via the silhouette statistics

```{r}
# see $cluster for cluster labels, $centers for centers, $withinss for clusterwise within sum of squares,
# $size for numbers of points in clusters, $bestk for optimal number of clusters, $crit averaged silhouette 
# for each nb of clusters which have been tested 

kmeans.zinb <- fpc::kmeansruns ( data = simdata.zinb$U , krange = c(2:5) , criterion = "asw" )
kmeans.pca <- fpc::kmeansruns ( data = simdata.pca$x[,1:2] , krange = c(2:5) , criterion = "asw" )
print(c(kmeans.zinb$bestk,kmeans.pca$bestk))
print(c(kmeans.zinb$withinss,kmeans.pca$withinss))
print(c(kmeans.zinb$crit,kmeans.pca$crit))
```





Decisions to take about the plan of simulations:

* Define and list all configurations which we want to simulate. Some suggestions: 
    + 2 latent factors without known design matrix (PCA case)
    + Should we consider more than 2 factors? Do we have a solution to choose the nb of factors?
    + Case of different library sizes ? Our way to normalize versus Deseq?
    + Any others?

* Number of datasets to simulate for each configuration. Suggestion: 
    + 50 datasets

* Number of genes, number of cells : 
    + reasonable values? 
    + one or several configuration per simulated case?

* Zero inflation
    + Should we consider the case of no zero inflation?
    + Which configurations of zero inflation we consider?
        + Several proportions of zeros: which ones?
    
* Define a list of methods to compare with :
    + ZIFA
    + PCA
    + Weighted PCA
    + Any others ? Should we compare to non linear methods:
        + Kernel PCA
        + Isomap
        + Maximum Variance Unfolding
        + Diffusion Maps
        + Locally Linear Embedding
        + Laplacian Eigenmaps

* Define criteria of comparison :
    + Error of reconstruction of M matrix? Error of reconstruction of Pi matrix?
    + General measures of quality of dimension reduction ? (without notion of clusters)
    + Dimension reduction + clustering. Suggestions:
        + For each method, do a dimension reduction followed by k-means, including the choice of number of clusters.
            1. Case of simulation with a known reference partition: compare Rand indices of similarity with ref
            2. General statistics on quality of clustering. Which ones? Package fpc, function cluster.stats provides:
                + Vector of clusterwise within cluster distance medians
                + Vector of cluster diameters
                + Vector of clusterwise minimum distances of a point in the cluster to a point of another cluster
                + Vector of clusterwise average distances of a point in the cluster to the points of other clusters
                + Matrix of separation values between all pairs of clusters
                + Matrix of mean dissimilarities between points of every pair of clusters
                + Average distance between clusters
                + Average distance within clusters
                + A generalisation of the within clusters sum of squares (k-means objective function)
                + Vector of cluster average silhouette widths
                + Average silhouette width
                + Silhouette averaged over all observations ( any better solution using silhouette? )
                + Minimum average dissimilarity between two cluster / maximum average within cluster dissimilarity
                + Average.within/average.between
                


