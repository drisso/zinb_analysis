---
title: "Orthogonalization of U by SVD"
output: html_document
---

## How to orthogonalize efficiently
Consider the problem of computing the SVD of a matrix $UV^\top$. 
In order to skip the multiplication of matrices, use the QR decomposition:
$$U=U_QU_R$$
where $U_Q$ is orthogonal of the same size as $U$,  and $U_R$ is upper triangular $2\times 2$. We have:
$$UV^T=U_QU_RV_R^TV_Q^T,$$
with $U_RV_R^T$ of size $2\times 2$. The SVD of $A:=U_RV_R^T$ is
$$A=A_UA_DA_V^T,$$
with $A_U,A_V$ orthogonals and $A_D$ diagonal matrices. This gives:
$$UV^T=U_QA_UA_DA_V^TV_Q^T=:\tilde{U}\tilde{V}^T,$$
where $\tilde{U}=U_QA_U$ and $\tilde{V}=V_Q A_V A_D$ are orthogonal. To keep the same $UW^T$, modifie $W$:
$$UW^T=U_QU_RW^T=U_QA_UA_U^TU_RW^T=\tilde{U}\tilde{W}^T,$$
with $\tilde{W}=WU_R^TA_U$. JP: in this version we do not need to inverse the diagonal and I do not use "solve" any more.

## Modification (JP)
Let's remember that we maximize the log-likelihood $\ell(U,V,W)$ penalized by the Frobenius norm of the matrices:
$$
\max_{U,V,W} \ell(U,V,W) - \epsilon(||U||^2+||V||^2 + ||W||^2) \,.
$$
The likelihood $\ell(U,V,W)$ only depends on $U,V,W$ through $UV^\top$ and $UW^\top$, so we can rewrite our objective function as
$$
\max_{U,V,W} f(UV^\top,UW^\top) - \epsilon(||U||^2+||V||^2 + ||W||^2) \,.
$$
The regularization is not only beneficial for statistical and numerical reasons, but also has the effect of naturally enforcing orthogonality of columns in the matrices. More precisely, forget about $W$ for a moment and consider the problem
$$
\max_{U,V} f(UV^\top) - \epsilon(||U||^2+||V||^2) \,.
$$
It is well known that the trace norm of a matrix $M$ (i.e., the sum of singular values) can be expressed as
$$
||M||_{\ast} = \min_{UV^\top=M}\left(||U||^2 + ||V||^2\right)
$$
so our problem becomes
$$
\max_M f(M) - \epsilon ||M||_{\ast} \,,
$$
and if $M^*$ denotes the solution of this problem then the $U^*$ and $V^*$ that achieve the maximum are the singular vectors of $M^*$ scaled by the square roots of the singular values: if $M^*=ASB^\top$ is the SVD of $M^*$, then $U^* = A\sqrt(S)$ and $V^*=B\sqrt{S}$. In other words, the optimal $U^*$ and $V^*$ are (scaled) singular vectors of $M$, and in particular have orthogonal columns.

Back to our problem with $W$, the same reasoning applies to the matrices $U$ and $[V;W]$, i.e., at the optimum the columns or U are orthogonal, as well as the columns of $[V,W]$.

In practice we optimize by alternating optimization in $U$ on the one hand, and $[V;W]$ on the other hand. To speed up convergence it can therefore be useful to insert an orthogonalization step between each optimization, where $U$ and $[V;W]$ are set to the scaled singular vectors of the matrix $M = U*[V;W]^\top$; this amounts to minimizing the regularization $(||U||^2+||V||^2 + ||W||^2)$ under the constraint that $\ell(U,V,W)$ does not change, and is obtained by a simple SVD.

This orthogononatization is implemented in the orthogonalizeJointly(U,V,W) function.

Note that the columns of $V$ only have no reason to be orthogonal, neither the columns of $W$ only. Hence the procedure presented in the first paragraph above (enforce orthogonal columns for $U$ and $V$, but not $W$) is not correct, in the sense that it degrades the objective function. This explains why we observed some oscillation in the objective function when we tried it. Instead, when $V$ and $W$ are treated jointly, then the orthogonalization step really corresponds to a partical maximization of the objective function.

## Test of the orthogonalization step

Test on small simulated matrices that this function does what it should

```{r}
library(zinb)
set.seed(1234)

# random matrices
U <- matrix( rnorm(16), ncol=2 )
V <- matrix( rnorm(20), ncol=2 )
W <- matrix( rnorm(20), ncol=2 )

# orthogonalize them
o <- orthogonalizeJointly(U,V,W)

# Check that UV' is conserved
sum((U%*%t(V) - o$U %*% t(o$V))^2)

# Check that UW' is conserved
sum((U%*%t(W) - o$U %*% t(o$W))^2)

# Check that U has orthogonal columns
t(o$U) %*% o$U

# Check that the regularization term has decreased
sum(U^2) + sum(V^2) + sum(W^2)
sum(o$U^2) + sum(o$V^2) + sum(o$W^2)
```

## Test on Fluidigm data
This procedure is included in the function called zinb.PCA.svd. (JP: I also changed initialization to SVD of log(count+1) instead of PCA(logcount+1). Let's run it on Fluidigm data
```{r, echo=FALSE}
library(SummarizedExperiment)
library(scRNAseq)
data("fluidigm")
high.cov.cells=assays(fluidigm)$counts[,which(colData(fluidigm)$Coverage_Type=="High")]
filter.out=apply(high.cov.cells>10,1,sum)<10
fluidigm2=t(high.cov.cells[!filter.out,])
zinb.orthog <- zinb.PCA.svd(fluidigm2,no_cores=7,alt.number=25,verbose=TRUE)
```
Check orthogonality ($U$ has orthogonal columns, but not $V$ or $W$)
```{r}
t(zinb.orthog$U)%*%zinb.orthog$U
t(zinb.orthog$V)%*%zinb.orthog$V
t(zinb.orthog$W)%*%zinb.orthog$W
```

Plot the result:
```{r}
plot(zinb.orthog$U,col=colData(fluidigm)$Cluster2[colData(fluidigm)$Coverage_Type=="High"],main="U")
plot(zinb.orthog$V,main="V")
plot(zinb.orthog$W,main="W")
plot(zinb.orthog$theta,main="theta")
```





