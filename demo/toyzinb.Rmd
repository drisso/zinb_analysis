---
title: "Test Zinb"
author: "Jean-Philippe Vert"
date: "24 mars 2016"
output: html_document
---

Here we demonstrate the use of ZINB on a toy dataset. 

# Standard PCA

Let us first look at the toy data using standard PCA of the log(count+1) matrix

```{r message=FALSE}
library(zinb)
pc <- prcomp(log(1+toydata),center=TRUE,scale.=TRUE)
plot(pc$x[,1:2])
```

We see two well-separated clusters, mostly separated along the first principal component. 

# Original ZINB

Let us now apply ZINB with two factors, and show that the likelihood increases at each iteration (after a small bug correction to make sure this is the case). This is the original function in the functions_svetlana.R file

```{r message=FALSE, eval=TRUE}
source("../temporary_files/functions_svetlana.R")
ptm <- proc.time()
z <- zinb(toydata, alt.number=10)
print(proc.time()-ptm)

plot(z$U)
```

We have to wait a few minutes, and there is a strange outlier in the final picture.

# ZINB v2: faster and numerically more stable

We wrote a faster and hopefully more stable version. The main changes compared to the original zinb are:

* only one row/column of the count matrix is passed to the likelihood function and its gradient (this is what makes the speed-up)
* the computation of the likelihood and its gradient have been modified to reduce the impact of numerical errors; this prevents the divergence of some of the coefficients whose likelihood could not get very small because of numerical errors in the original version.

```{r message=FALSE}
ptm <- proc.time()
z2 <- zinb2(toydata, alt.number=10)
print(proc.time()-ptm)
plot(z2$U)
```

We see that the new version converges faster to an apparently better solution. The speed-up is roughly 3.8x.

# ZINB v3 : faster, more stable, and ready for more general regression

We made a new version, again slightly faster, hopefully more stable, and more importantly ready to be used for the general regression framework with various covariates (which is the main motivation to rewrite a new function). Compared to the previous one:

* the two likelihood and gradient functions (one for the left optimization, one for the right optimization) have been merged onto a single function that can be called with various arguments
* the function can perform not only PCA, but the more general regression settings with covariates and offsets
* the gradient of the likelihood has been improved for numerical stability
* unnecessary computations have been removed in the computation of the gradient and its likelihood (which leads to a small speed up).
* the main embarassingly parallel loop has been parallelized

Let us first check it withough parallelization
```{r message=FALSE}
ptm <- proc.time()
z3 <- zinb.PCA(toydata, alt.number=10, verbose=TRUE, no_cores=1)
print(proc.time()-ptm)
plot(z3$U)
```
Not bad, we have a 1.5x speed-up compared to the previous version on a single core (and 5.8x compared to the initial version), and we obtain the same result (visually). Let us now run the parallel version (by default, without the no_cores argument, the function will use all the cores of your machine minus one):
```{r message=FALSE}
ptm <- proc.time()
z4 <- zinb.PCA(toydata, alt.number=10, verbose=TRUE, no_cores=3)
print(proc.time()-ptm)
plot(z4$U)
```

On an 8-core machines, running on 7 cores, we observe a further 3.8x speed-up. More careful inspection of the code (notably how variables are passed) may lead to a speed-up closer to the maximum 7x speed-up that we could expect, but for the moment it is not too bad.

Note that if we do not optimize over theta, whose gradient is the most expensive to compute, we should get further speed-up. The last version does that for free, it suffices to change the arguments of the optimization function (TODO...)