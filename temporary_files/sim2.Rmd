---
title: "Simulation results"
author: "Davide Risso"
date: '`r Sys.Date()`'
output:
  html_document
---

```{r options, echo=FALSE, results="hide",mesasge=FALSE, error=FALSE,include=FALSE}
knitr::opts_chunk$set(fig.align="center", cache=TRUE, error=FALSE, message=FALSE, echo=TRUE, warning=FALSE,results="markup")
```

Here I simulate two populations, and I compare the estimates of my model to Michael's model, especially in terms of weights for PCA.


```{r sim, echo=FALSE}
source("../R/standard.R")
source("../R/reg_smallmatrix.R")
source("../R/em.R")
source("~/git/YosefCode/packages/RNASeq/OFBIT/SCONE/R/estimateFNR.R")

## true expression: from BRAIN data
set.seed(1893)
library(scde)
library(MASS)
library(matrixStats)
library(brainData)
data(brain01_counts)

filteredCells <- read.table("~/git/brainAnalysis/brain/data/brain01_filtered_cells.txt", as.is=TRUE)

exclude <- !(brain01_counts$Barcode %in% filteredCells[,1])
brain01 <- brain01_counts[,!exclude]

counts <- exprs(brain01)
type <- as.factor(brain01$"Cell.type")

genes <- rownames(brain01)[fData(brain01)[,3]==1]
genes <- intersect(genes, rownames(na.omit(counts)))

filtered <- filterCount(counts[genes,], nRead=10, nCell=10)

geneMap <- read.table("~/git/brainAnalysis/brain/data/gene.map", row.names=1)

ens <- rownames(geneMap)
names(ens) <- geneMap[,1]
allgenes <- ens[rownames(filtered)]

allgenes <- na.omit(allgenes)
info <- read.table("~/git/brainAnalysis/brain/data/gene.info", row.names=1)
len <- info[allgenes,1]
gcc <- info[allgenes,2]
names(len) <- names(gcc) <- names(allgenes)

allgenes <- allgenes[names(na.omit(len))]

len <- len[names(allgenes)]
gcc <- gcc[names(allgenes)]
filtered <- filtered[names(allgenes),]

filtered0 <- filtered
filtered0[filtered==0] <- NA

means <- round(rowMedians(filtered0, na.rm=TRUE))
names(means) <- rownames(filtered0)

J <- 5000
## n must be even
n <- 100

## first 500 genes are DE
nde <- 500
lfc <- rep(0, J)
lfc[1:nde] <- sample(1:4, nde, replace=TRUE) * (-1)^rbinom(nde, 1, .5)    

## two populations
mu_pop <- sample(means, J)
mu0 <- exp(log(mu_pop) - 0.5 * lfc)
mu1 <- exp(log(mu_pop) + 0.5 * lfc)

mu <- cbind(matrix(data=rep(mu0, round(n/2)), ncol=round(n/2), nrow=J),
            matrix(data=rep(mu1, round(n/2)), ncol=round(n/2), nrow=J))

## sample from real GCC and length
l <- len[names(mu_pop)]
g <- gcc[names(mu_pop)]

## values based on BRAIN data
alpha0 <- rnorm(n, mean=2, sd=2)
alpha1 <- rnorm(n, mean=-1, sd=0.3)
alpha2 <- rnorm(n, mean=0, sd=0.15)
alpha3 <- rnorm(n, mean=1.15, sd=2)

logistic <- binomial()$linkinv

## generate pi = prob. of "detection"
pi <- sapply(1:n , function(i) {
  eta <- alpha0[i] + alpha1[i] * log(mu[,i]) #+ alpha2[i] * log(l) + alpha3[i] * g
  return(logistic(eta))
})

## generate Z, indicator of expression
Z <- matrix(rbinom(n*J, 1, pi), ncol=n)

## generate Y, read counts (same dispersion for all genes)
Y <- matrix(data=0, ncol=n, nrow=J)
Y[Z==0] <- rnbinom(sum(Z==0), mu = mu[Z==0], size = 1)

group <- as.factor(c(rep(1, n/2), rep(2, n/2)))
cols <- c("steelblue4", "tan1")[group]

## true pi
plot(1-pi[order(mu[,1]),1]~sort(log(mu[,1])), type='l', ylim=c(0, 1), ylab="1 - pi", xlab="log(mu)")
for(i in 2:n) {
  lines(1-pi[order(mu[,i]),i]~sort(log(mu[,i])), col=rainbow(100)[i])
}

true_pca <- prcomp(t(log1p(Y)), center=TRUE, scale=TRUE)
plot(true_pca$x[,1:2], col=cols, pch=19)

pdf("pca.pdf")
plot(true_pca$x[,1:2], col=cols, pch=19)
dev.off()

w <- 1 - (Y == 0) * (pi/(pi+(1-pi)*(1+mu)^(-1)))
true_wpca <- bwpca(t(log1p(Y)), t(w))
plot(true_wpca$scores[,1:2], col=cols, xlab="wPC1", ylab="wPC2", pch=19)

Yimputed <- Y * w + mu * (1 - w)
true_imputed <- prcomp(t(log1p(Yimputed)), center=TRUE, scale=TRUE)
plot(true_imputed$x[,1:2], col=cols, pch=19)

variance <- (1 - pi) * mu * (1 + mu * (1 + pi))

plot(true_pca$x[,1]~colMeans(pi), col=cols, xlab="Average Pi", ylab="PC1")
plot(true_wpca$scores[,1]~colMeans(pi), col=cols, xlab="Average Pi", ylab="wPC1")
```

```{r em, depenson="sim"}
print(system.time(fit1 <- em(Y, maxiter=10)))

plot(1-fit1$pihat[order(fit1$muhat),1]~sort(log(fit1$muhat)), type='l', ylim=c(0, 1), ylab="1 - pihat", xlab="log(muhat)")
for(i in 2:n) {
  lines(1-fit1$pihat[order(fit1$muhat),i]~sort(log(fit1$muhat)), col=rainbow(100)[i])
}

what1 <- 1 - (Y == 0) * fit1$pihat/(fit1$pihat+(1-fit1$pihat)*(1+fit1$muhat/fit1$thetahat)^(-fit1$thetahat))
wpca1 <- bwpca(t(log1p(Y)), t(what1))
plot(wpca1$scores[,1:2], col=cols, xlab="wPC1", ylab="wPC2", pch=19)

pdf("wpca.pdf")
plot(wpca1$scores[,1:2], col=cols, xlab="wPC1", ylab="wPC2", pch=19)
dev.off()
```

```{r scone, depenson="sim"}
print(system.time(fit_scone <- estimate_zinb(Y)))

wpca_scone <- bwpca(t(log1p(Y)), t(fit_scone$p_z))
plot(wpca_scone$scores[,1:2], col=cols, xlab="wPC1", ylab="wPC2", pch=19)

pdf("wpca_scone.pdf")
plot(wpca_scone$scores[,1:2], col=cols, xlab="wPC1", ylab="wPC2", pch=19)
dev.off()
```

```{r fnr, dependson="sim"}
hk <- 501:1000
system.time(fnr_out <- estimateFNR(Y, bulk_model = TRUE, is.expressed = hk))

plot(fnr_out$P[order(fnr_out$Alpha[1,]), 1] ~ sort(fnr_out$Alpha[1,]), type='l', ylim=c(0, 1), ylab="pihat", xlab="log(muhat)")
for(i in 2:ncol(Y)) {
  lines(fnr_out$P[order(fnr_out$Alpha[1,]), i] ~ sort(fnr_out$Alpha[1,]), col=rainbow(100)[i])
}

what2 <- 1 - (Y == 0) * (fnr_out$Z)
wpca2 <- bwpca(t(log1p(Y)), t(what2))
plot(wpca2$scores[,1:2], col=cols, xlab="wPC1", ylab="wPC2")
```

```{r compare, dependson=c("em", "fnr")}
boxplot(data.frame(EM=as.numeric(fit1$pihat - pi), FNR=as.numeric(1-fnr_out$Z[,1] - pi)), main="Pi", ylab="pihat - pi")
abline(h=0, col=2)

boxplot(data.frame(EM=as.numeric(what1 - w), FNR=as.numeric(what2 - w)), main="Weights", ylab="bias")
abline(h=0, col=2)
```

## Conclusions (to be confirmed by real data and/or more extensive simulations?)

Although the full negative binomial model leads to better estimates of the "drop-out probability," for the purpose of weighted PCA Michael's FNR method seems "good enough."
In particular, in this simple simulation scenario, the first PC when using regular PCA relates to the detection rates, while wPCA works fine, both with the true weights and with weights inferred from the models.

